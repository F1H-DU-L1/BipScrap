# Skrypt Web Scraping

Ten skrypt su偶y do web scrapingu z u偶yciem Selenium oraz BeautifulSoup. Skrypt przeszukuje strony internetowe, zbiera tre z r贸偶nych podstron, a nastpnie zapisuje dane do plik贸w CSV, HTML oraz tworzy plik TXT z adresami przetworzonych podstron. Dodatkowo, skrypt oblicza hash SHA-256 dla wygenerowanych plik贸w i tworzy raport.

## .env
RABBITMQ_USER: ""
RABBITMQ_PASS: ""
RABBITMQ_HOST: ""
RABBITMQ_QUEUE: ""

## Instalacja

Upewnij si, 偶e masz zainstalowany Python w wersji 3.6 lub wy偶szej. Nastpnie uruchom poni偶sze polecenie, aby zainstalowa zale偶noci:

```bash
pip install -r requirements.txt
```

Skrypt wymaga nastpujcych pakiet贸w:

- `selenium`
- `beautifulsoup4`
- `requests`
- `hashlib`
- `time`
- `os`
- `csv`
<<<<<<< HEAD

## Opis funkcji

### 1. `get_soup_selenium(url)`
=======
- `json`
- `pika`
- `dotenv`
- `schedule`

## Opis funkcji

### 1. `send_to_rabbitmq(url, text)`
**Opis** Funkcja przesya do kolejki rabbitMQ adresy URL i zawarto stron

### 2. `get_soup_selenium(url)`
>>>>>>> 5f2f32b (Dodanie kolejki rabbitmq do moduu zbierania danych.)
**Opis**: Funkcja pobiera stron internetow za pomoc Selenium, czekajc na zaadowanie wszystkich element贸w strony, w tym tych dynamicznych adowanych przez JavaScript. Zwraca obiekt `BeautifulSoup` oraz surowy HTML.

**Wejcie**:
- `url`: Adres URL strony do pobrania.

**Wyjcie**:
- `soup`: Obiekt `BeautifulSoup` z zawartoci strony.
- `raw_html`: Surowy HTML strony.

<<<<<<< HEAD
### 2. `extract_main_content(soup, raw_html)`
=======
### 3. `extract_main_content(soup, raw_html)`
>>>>>>> 5f2f32b (Dodanie kolejki rabbitmq do moduu zbierania danych.)
**Opis**: Funkcja usuwa nag贸wki, stopki, paski nawigacyjne, skrypty oraz inne zbdne elementy ze strony i jej surowego HTML-a. Zwraca oczyszczon tre strony oraz oczyszczony HTML.

**Wejcie**:
- `soup`: Obiekt `BeautifulSoup` zawierajcy stron HTML.
- `raw_html`: Surowy HTML strony.

**Wyjcie**:
- `content`: Oczyszczona tre strony.
- `cleaned_raw_html`: Oczyszczony HTML strony.

<<<<<<< HEAD
### 3. `compute_file_hash(file_path)`
=======
### 4. `compute_file_hash(file_path)`
>>>>>>> 5f2f32b (Dodanie kolejki rabbitmq do moduu zbierania danych.)
**Opis**: Funkcja oblicza hash SHA-256 dla pliku wskazanego przez cie偶k.

**Wejcie**:
- `file_path`: cie偶ka do pliku, kt贸rego hash ma zosta obliczony.

**Wyjcie**:
- `file_hash`: Hash SHA-256 pliku.

<<<<<<< HEAD
### 4. `create_hash_report(csv_file, html_file, output_txt_file)`
=======
### 5. `create_hash_report(csv_file, html_file, output_txt_file)`
>>>>>>> 5f2f32b (Dodanie kolejki rabbitmq do moduu zbierania danych.)
**Opis**: Funkcja generuje raport w formacie tekstowym zawierajcy hashe plik贸w CSV oraz HTML.

**Wejcie**:
- `csv_file`: cie偶ka do pliku CSV.
- `html_file`: cie偶ka do pliku HTML.
- `output_txt_file`: cie偶ka do pliku, w kt贸rym zapisany zostanie raport hashy.

**Wyjcie**:
- Zapisuje raport hashy do pliku tekstowego.

<<<<<<< HEAD
### 5. `crawl(domain, base_url, max_pages_per_url=None)`
=======
### 6. `crawl(domain, base_url, max_pages_per_url=None)`
>>>>>>> 5f2f32b (Dodanie kolejki rabbitmq do moduu zbierania danych.)
**Opis**: Funkcja do przeszukiwania domeny i zbierania danych z podstron. Funkcja pobiera strony, oczyszcza tre, zapisuje dane do plik贸w CSV, HTML oraz mapy witryny. Dziaa rekurencyjnie, przetwarzajc wszystkie dostpne podstrony.

**Wejcie**:
- `domain`: Domena strony, kt贸ra ma zosta przeszukana.
- `base_url`: Podstawowy URL, od kt贸rego rozpoczyna si skanowanie.
- `max_pages_per_url` (opcjonalnie): Limit liczby podstron do przetworzenia dla ka偶dej domeny. Domylnie brak limitu (None).

**Wyjcie**:
- Zapisuje dane w plikach:
  - `scraped_<timestamp>.csv`: Zawiera URL i oczyszczon tre ka偶dej strony.
  - `scraped_<timestamp>.txt`: Zawiera surowy HTML ka偶dej strony.
  - `sitemap.txt`: Zawiera list przetworzonych URL.
  - `hash_file.txt`: Zawiera raport hashy dla plik贸w CSV i HTML.

<<<<<<< HEAD
### 6. `main()`
=======
### 7. `main()`
>>>>>>> 5f2f32b (Dodanie kolejki rabbitmq do moduu zbierania danych.)
**Opis**: Funkcja g贸wna uruchamiajca skrypt. Uruchamia przetwarzanie dla listy podanych URL-i.

**Wejcie**:
- `urls`: Lista URL-i do przetworzenia.
- `max_pages_per_url`: Limit liczby podstron do przetworzenia.

**Wyjcie**:
- Uruchamia proces crawlowania i zapisuje dane do odpowiednich plik贸w.

## Przykad u偶ycia

### Skrypt

W pliku `main.py` mo偶na ustawi list URL-i, kt贸re maj by przetworzone, oraz limit podstron.

```python
def main():
    """G贸wna funkcja uruchamiajca skrypt."""
    urls = [
        "http://quotes.toscrape.com/",
        "http://quotes.toscrape.com/js"
    ]
    
    max_pages_per_url = 5  # Limit podstron, np. 15. Mo偶na ustawi na None, aby nie byo limitu.

    for url in urls:
        print(f" Skanuj: {url}")
        crawl(url, url, max_pages_per_url)

    driver.quit()
```

### Uruchomienie skryptu

Aby uruchomi skrypt, po prostu uruchom plik Python:

```bash
python webscrapping.py
```

Skrypt przetworzy podane URL-e i zapisze wyniki do plik贸w w folderze `data/`, tworzc podfoldery w zale偶noci od daty i godziny uruchomienia.

## Pliki wyjciowe

Po zakoczeniu przetwarzania skrypt tworzy nastpujce pliki:

- **CSV**: `scraped_<timestamp>.csv`
  - Zawiera kolumny: URL, Content (oczytana tre strony).
  
- **HTML**: `scraped_<timestamp>.txt`
  - Zawiera peny surowy HTML stron.
  
- **Mapa witryny**: `sitemap.txt`
  - Zawiera list URL-i przetworzonych podczas skanowania.
  
- **Raport hashy**: `hash_file.txt`
  - Zawiera SHA-256 hashe dla plik贸w CSV i HTML.

## Ustawienia

### `urls`:
- Lista URL-i, kt贸re maj zosta przetworzone.

### `max_pages_per_url`:
- Ogranicza liczb podstron do przetworzenia z danej domeny. Mo偶na ustawi na `None`, aby nie byo limitu.

### Przykad raportu hashy w `hash_file.txt`:

```
Plik: data/quotes_toscrape_com_20250330/scraped_20250330.csv
Hash: d2d2d2a55c4a52e4eb2e28a7cd582c6a15d1b9da59302a512ef47ba1f34a4045

Plik: data/quotes_toscrape_com_20250330/scraped_20250330.txt
Hash: f1c1e0aaf343a82f8a87cc8c1ac6d85fc7393a274890d8f9837172dbba3d45d0
```

## Licencja

Ten skrypt jest udostpniany na licencji MIT.
